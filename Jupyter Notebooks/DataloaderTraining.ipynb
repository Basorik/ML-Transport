{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATALOADER TRAINING\n",
    "\n",
    "This script is the basic training script for the Pytorch based machine learning\n",
    "\n",
    "'torch' and 'torchvision' are the main libraries in use here and are parts of the Pytorch machine learning library. Multiple parts of this library are imported separately because it was easier, but this could be cleaned up\n",
    "\n",
    "\n",
    "The 'device' variable is setting the device used to run the machine learning to either a CUDA based GPU or a CPU depending on what is available. A CUDA GPU will be many times faster to run on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section sets up the Dataset. This is done using the 'ImageFolder' function within Pytorch. This takes in a folder structured in a particular way, and converts it into a Pytorch dataset that uses tensors.  \n",
    "Next this dataset is split into two channels, train and validation. The sizes of the channels can be changed here using the numbers in the 'lengths' variable which are currently 0.8 and 0.2 respectivly  \n",
    "The two channels are then turned into Dataloaders, which control the batch size, shuffling and number of parallel workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'C:\\\\Users\\\\zshal\\\\Documents\\\\Pytorch\\\\Images\\\\Images\\\\v3'\n",
    "data = ImageFolder(root=base, transform=ToTensor())\n",
    "lengths = ((int)(len(data)*0.8), (int)(len(data)*0.2))\n",
    "trainsplit, testsplit = torch.utils.data.random_split(data, lengths)\n",
    "\n",
    "trainimages = DataLoader(trainsplit, batch_size=4, shuffle=True, num_workers=0)\n",
    "testimages = DataLoader(testsplit, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "classes = ('pothole', 'no pothole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section the model itself is described using a class. The descriptions are hopefully pretty straightforward, but the _init_ section is setting up the layers that will be used, and the 'forward' section is showing how the layers fit together. This example has been kept basic, but should have a decent balance of accuracy and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #self.conv2 = nn.Conv2d(12, 12, 3)\n",
    "        #self.fc1 = nn.Linear(555984, 120)\n",
    "        self.fc1 = nn.Linear( 12*399*471, 360)\n",
    "        self.fc2 = nn.Linear( 360, 120)\n",
    "        self.fc3 = nn.Linear(120, 84)\n",
    "        self.fc4 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #x = self.pool(F.relu(self.conv2(x)))\n",
    "        #x = x.view(-1, 12*198*234)\n",
    "        x = x.view(-1, 12*399*471)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the loss fucntion and targets are set. These can be changed to other loss functions and optimisers to change how the model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the training loop section. This should be relativly self explanitory, but the number of epochs can be changed here in the for loop. The printing style can also be changed if more detailed logs are requried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(6):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainimages, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        print(i)\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the output of the training is saved to a .pth file. This can be loaded into other scripts and it is every important that it gets saved properly. In this case the loading back is shown too as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './conv1CrossValidation6.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the trained net is used on the validation dataset to check accuracy for both the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in testimages:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "with torch.no_grad():\n",
    "    for data in testimages:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(2):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
